{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moritz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from argsum import get_quality_scores\n",
    "from collections import defaultdict\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArgKP21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ArgKP21 = pd.read_csv('data/ArgKP-2021/dataset.csv')\n",
    "\n",
    "##############################################\n",
    "### Add train / dev / test set information ###\n",
    "##############################################\n",
    "\n",
    "# Add train/dev/test split information based in KPA 2021 Shared Task data\n",
    "topics_train = pd.read_csv('data/KPA_2021_shared_task/kpm_data/arguments_train.csv').topic.unique()\n",
    "topics_dev = pd.read_csv('data/KPA_2021_shared_task/kpm_data/arguments_dev.csv').topic.unique()\n",
    "topics_test = pd.read_csv('data/KPA_2021_shared_task/test_data/arguments_test.csv').topic.unique()\n",
    "conditions = [\n",
    "    ArgKP21.topic.isin(topics_train),\n",
    "    ArgKP21.topic.isin(topics_dev),\n",
    "    ArgKP21.topic.isin(topics_test)\n",
    "    ]\n",
    "values = ['train', 'dev', 'test']\n",
    "ArgKP21['set'] = np.select(conditions, values)\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = ArgKP21['argument'].to_list(), \n",
    "                                    topic = ArgKP21['topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(ArgKP21['argument'].to_list()))\n",
    "\n",
    "ArgKP21.insert(len(ArgKP21.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "ArgKP21.to_csv('data/ArgKP-2021/dataset_splits_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ArgKP21 = pd.read_csv('data/ArgKP-2021/dataset_splits_scores.csv')\n",
    "\n",
    "##############################################\n",
    "### Remove arguments that do not have ########\n",
    "### exactly one matching key point or that ###\n",
    "### consists of more than one sentence #######\n",
    "##############################################\n",
    "\n",
    "ArgKP21_splits_processed = pd.DataFrame(columns = ArgKP21.columns)\n",
    "punkt_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for arg in ArgKP21['argument'].unique():\n",
    "    sentences = punkt_sentence_tokenizer.tokenize(arg)\n",
    "    if len(sentences) == 1:\n",
    "        data = ArgKP21[ArgKP21['argument'] == arg]\n",
    "        if data['label'].sum() == 1:\n",
    "            ArgKP21_splits_processed = pd.concat([ArgKP21_splits_processed, data[data['label'] == 1]], ignore_index = True)\n",
    "ArgKP21_splits_processed.to_csv('data/ArgKP-2021/dataset_splits_scores_processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArgKP23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient:   0%|          | 0/9281 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 9281/9281 [01:49<00:00, 84.62it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ArgKP23 = pd.read_csv('data/ArgKP-2023/dataset.csv')\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = ArgKP23['argument'].to_list(), \n",
    "                                    topic = ArgKP23['topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(ArgKP23['argument'].to_list()))\n",
    "\n",
    "ArgKP23.insert(len(ArgKP23.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "ArgKP23.to_csv('data/ArgKP-2023/dataset_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArgQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 30497/30497 [06:07<00:00, 83.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ArgQ = pd.read_csv('data/IBM-ArgQ-Rank-30kArgs/dataset.csv')\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = ArgQ['argument'].to_list(), \n",
    "                                    topic = ArgQ['topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(ArgQ['argument'].to_list()))\n",
    "\n",
    "ArgQ.insert(len(ArgQ.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "ArgQ.to_csv('data/IBM-ArgQ-Rank-30kArgs/dataset_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear Energy (Summetix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 339/339 [00:02<00:00, 126.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "nuclear_energy = pd.read_csv('data/Summetix/NuclearEnergy_clusteringEval.tsv', sep = '\\t')\n",
    "topic = 'Nuclear Energy'\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = nuclear_energy['argument'].to_list(), \n",
    "                                    topic = topic,\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(nuclear_energy['argument'].to_list()))\n",
    "\n",
    "nuclear_energy.insert(len(nuclear_energy.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "nuclear_energy.to_csv('data/Summetix/NuclearEnergy_clusteringEval_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debatepedia (Summetix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 1804/1804 [00:20<00:00, 87.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "debatepedia = pd.read_csv('data/Summetix/debatepedia_processed_9Topics.tsv', sep = '\\t')\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = debatepedia['argument'].to_list(), \n",
    "                                    topic = debatepedia['topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(debatepedia['argument'].to_list()))\n",
    "\n",
    "debatepedia.insert(len(debatepedia.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "debatepedia.to_csv('data/Summetix/debatepedia_processed_9Topics_scores_topic.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 1804/1804 [00:20<00:00, 87.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "debatepedia = pd.read_csv('data/Summetix/debatepedia_processed_9Topics.tsv', sep = '\\t')\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = debatepedia['argument'].to_list(), \n",
    "                                    topic = debatepedia['sub_topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(debatepedia['argument'].to_list()))\n",
    "\n",
    "debatepedia.insert(len(debatepedia.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "debatepedia.to_csv('data/Summetix/debatepedia_processed_9Topics_scores_sub_topic.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 1804/1804 [00:20<00:00, 86.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "debatepedia = pd.read_csv('data/Summetix/debatepedia_processed_9Topics.tsv', sep = '\\t')\n",
    "topic = [debatepedia['topic'].to_list()[i] + ' ' + debatepedia['sub_topic'].to_list()[i] for i in range(len(debatepedia))]\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = debatepedia['argument'].to_list(), \n",
    "                                    topic = topic,\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(debatepedia['argument'].to_list()))\n",
    "\n",
    "debatepedia.insert(len(debatepedia.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "\n",
    "# Save the processed dataset\n",
    "debatepedia.to_csv('data/Summetix/debatepedia_processed_9Topics_scores_concatenation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate dataset\n",
    "https://aclanthology.org/D14-1083/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to create a pandas dataframe\n",
    "def removeprefix(str, prefix):\n",
    "    if str.startswith(prefix):\n",
    "        return str[len(prefix):]\n",
    "    else:\n",
    "        return str\n",
    "\n",
    "def get_args_kp_by_topic_debate():\n",
    "    path = 'data/Debate/reason/'\n",
    "    folders = ['abortion', 'gayRights', 'marijuana', 'obama']\n",
    "\n",
    "    arguments_by_topic = defaultdict(dict)\n",
    "\n",
    "    for folder in folders:\n",
    "        filenames = next(walk(path+folder), (None, None, []))[2]\n",
    "        arguments_by_topic[folder] = {}\n",
    "        args_by_label = defaultdict(list)\n",
    "        for filename in filenames:\n",
    "            with open(path+folder+'/'+filename, encoding='utf-8', errors='ignore') as f:\n",
    "                label = None\n",
    "                for line in f:\n",
    "                    if 'Label##' in line:\n",
    "                        label = removeprefix(line, 'Label##')\n",
    "                        label = label.strip()\n",
    "                    if 'Line##' in line:\n",
    "                        text = removeprefix(line, 'Line##')\n",
    "                        text = text.strip()\n",
    "                        args_by_label[label].append(text)\n",
    "        arguments_by_topic[folder] = args_by_label\n",
    "\n",
    "    path = 'data/Debate/reason/labels/'\n",
    "    for folder in folders:\n",
    "        labels = []\n",
    "        with open(path+folder+'.txt', encoding='utf-8', errors='ignore') as f:\n",
    "            file_list = []\n",
    "            for line in f:\n",
    "                if line.strip(): file_list.append(line.strip())\n",
    "            for i in range(int(len(file_list)/2)):\n",
    "                if file_list[i*2] == 'p-other':\n",
    "                    arguments_by_topic[folder]['Pro Other'] = arguments_by_topic[folder].pop('p-other')\n",
    "                elif file_list[i*2] == 'p-Other':\n",
    "                    arguments_by_topic[folder]['Pro Other'] = arguments_by_topic[folder].pop('p-Other')\n",
    "                elif file_list[i*2] == 'c-other':\n",
    "                    arguments_by_topic[folder]['Con Other'] = arguments_by_topic[folder].pop('c-other')\n",
    "                elif file_list[i*2] == 'c-Other':\n",
    "                    arguments_by_topic[folder]['Con Other'] = arguments_by_topic[folder].pop('c-Other')\n",
    "                else:\n",
    "                    arguments_by_topic[folder][file_list[i*2+1]] = arguments_by_topic[folder].pop(file_list[i*2])\n",
    "                    \n",
    "    for folder in folders:\n",
    "        arguments_by_topic[folder].pop('Pro Other')\n",
    "        arguments_by_topic[folder].pop('Con Other')\n",
    "    \n",
    "    return arguments_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract summaries with corresponding stances\n",
    "path = 'data/Debate/reason/labels/'\n",
    "folders = ['abortion', 'gayRights', 'marijuana', 'obama']\n",
    "summaries = []\n",
    "stances = []\n",
    "stance_dict = {'p':1,'c':-1}\n",
    "for folder in folders:\n",
    "    with open(path+folder+'.txt', encoding='utf-8', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line[0:2] in ['p-', 'c-']:\n",
    "                stances.append(stance_dict[line[0:1]])\n",
    "            elif line[0:2] not in ['p-', 'c-', '\\n']:\n",
    "                summaries.append(line.strip())  \n",
    "summaries_with_stances = dict(zip(summaries, stances))\n",
    "summaries_with_stances.pop('Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ArgumentQualityClient: 100%|██████████| 3228/3228 [00:37<00:00, 87.24it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "debate = get_args_kp_by_topic_debate()\n",
    "\n",
    "debate_df = pd.DataFrame(columns = ['topic', 'argument', 'summary'])\n",
    "for topic in list(debate.keys()):\n",
    "    summaries = list(debate[topic].keys())\n",
    "    for summary in summaries:\n",
    "        arguments = debate[topic][summary]\n",
    "        debate_df = pd.concat([debate_df, pd.DataFrame({'topic':[topic for i in range(len(arguments))],\n",
    "                                                        'argument': arguments,\n",
    "                                                        'summary': [summary for i in range(len(arguments))],\n",
    "                                                        'stance': [summaries_with_stances[summary] for i in range(len(arguments))]})], ignore_index = True)\n",
    "#debate_df.insert(loc = len(debate_df.columns), column = 'stance', value = [0 for i in range(len(debate_df))])\n",
    "\n",
    "##############################################\n",
    "### Add IBM Project Debater's Quality Scores #\n",
    "##############################################\n",
    "\n",
    "quality_scores = get_quality_scores(model = 'debater_api',\n",
    "                                    arguments = debate_df['argument'].to_list(), \n",
    "                                    topic = debate_df['topic'].to_list(),\n",
    "                                    sleep_time = 0,\n",
    "                                    n = len(debate_df['argument'].to_list()))\n",
    "\n",
    "debate_df.insert(len(debate_df.columns), 'ibm_pro_deb_qs', quality_scores)\n",
    "debate_df = debate_df.astype({'stance':int})\n",
    "\n",
    "# Save the processed dataset\n",
    "debate_df.to_csv('data/Debate/dataset_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "debate_scores = pd.read_csv('data/Debate/dataset_scores.csv')\n",
    "\n",
    "##############################################\n",
    "### Remove arguments that do not have ########\n",
    "### exactly one matching key point or that ###\n",
    "### consists of more than one sentence #######\n",
    "##############################################\n",
    "\n",
    "debate_scores_processed = pd.DataFrame(columns = debate_scores.columns)\n",
    "punkt_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for arg in debate_scores['argument'].unique():\n",
    "    sentences = punkt_sentence_tokenizer.tokenize(arg)\n",
    "    if len(sentences) == 1:\n",
    "        data = debate_scores[debate_scores['argument'] == arg]\n",
    "        if len(data) == 1:\n",
    "            debate_scores_processed = pd.concat([debate_scores_processed, data], ignore_index = True)\n",
    "debate_scores_processed.to_csv('data/Debate/dataset_scores_processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "Debate_processed = pd.read_csv('data/Debate/dataset_scores_processed.csv')\n",
    "Debate_processed_test = pd.DataFrame()\n",
    "\n",
    "summaries = Debate_processed['summary'].unique()\n",
    "for sum in summaries:\n",
    "    mask = (Debate_processed['summary'] == sum)\n",
    "    Debate_processed_test = pd.concat([Debate_processed_test, Debate_processed[mask].sample(frac = 0.5, random_state = 3845)])\n",
    "Debate_processed_test.to_csv('data/Debate/dataset_scores_processed_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
