{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan T5 Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/HarrywillDr/KeyPoint-Analysis/blob/main/KPG/kpa_t5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritz/miniconda3/envs/AM/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/moritz/miniconda3/envs/AM/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading configuration file config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from argsum import load_kp_test, preprocess_input_t5, train_t5, get_t5_cluster_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         ?, ?it/s].90it/s]\r"
     ]
    }
   ],
   "source": [
    "# Load ArgKP21\n",
    "df = pd.read_csv('data/ArgKP-2021/dataset_splits.csv')\n",
    "# Load ArgKP21 train/dev splits\n",
    "df_train = df[df['set'] == 'train']\n",
    "df_dev = df[df['set'] == 'dev']\n",
    "# Load KP_test\n",
    "KP_test = load_kp_test()\n",
    "# Preprocess ArgKP21 train/dev splits\n",
    "df_train = preprocess_input_t5(df_train)\n",
    "df_dev = preprocess_input_t5(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.3621, 'learning_rate': 0.000358974358974359, 'epoch': 1.54}\n",
      "{'eval_loss': 0.5509318113327026, 'eval_runtime': 1.1974, 'eval_samples_per_second': 30.064, 'eval_steps_per_second': 2.505, 'epoch': 1.54}\n",
      "{'loss': 0.3898, 'learning_rate': 0.0003179487179487179, 'epoch': 3.08}\n",
      "{'eval_loss': 0.14726188778877258, 'eval_runtime': 1.1434, 'eval_samples_per_second': 31.486, 'eval_steps_per_second': 2.624, 'epoch': 3.08}\n",
      "{'loss': 0.1226, 'learning_rate': 0.00027692307692307695, 'epoch': 4.62}\n",
      "{'eval_loss': 0.1561894416809082, 'eval_runtime': 1.0047, 'eval_samples_per_second': 35.833, 'eval_steps_per_second': 2.986, 'epoch': 4.62}\n",
      "{'loss': 0.0737, 'learning_rate': 0.00023589743589743593, 'epoch': 6.15}\n",
      "{'eval_loss': 0.17279836535453796, 'eval_runtime': 1.0187, 'eval_samples_per_second': 35.337, 'eval_steps_per_second': 2.945, 'epoch': 6.15}\n",
      "{'loss': 0.043, 'learning_rate': 0.00019487179487179487, 'epoch': 7.69}\n",
      "{'eval_loss': 0.20564384758472443, 'eval_runtime': 1.0222, 'eval_samples_per_second': 35.218, 'eval_steps_per_second': 2.935, 'epoch': 7.69}\n",
      "{'loss': 0.0275, 'learning_rate': 0.00015384615384615385, 'epoch': 9.23}\n",
      "{'eval_loss': 0.22555463016033173, 'eval_runtime': 1.2428, 'eval_samples_per_second': 28.967, 'eval_steps_per_second': 2.414, 'epoch': 9.23}\n",
      "{'loss': 0.0175, 'learning_rate': 0.00011282051282051283, 'epoch': 10.77}\n",
      "{'eval_loss': 0.239091694355011, 'eval_runtime': 1.3121, 'eval_samples_per_second': 27.437, 'eval_steps_per_second': 2.286, 'epoch': 10.77}\n",
      "{'loss': 0.0129, 'learning_rate': 7.17948717948718e-05, 'epoch': 12.31}\n",
      "{'eval_loss': 0.27382224798202515, 'eval_runtime': 1.4902, 'eval_samples_per_second': 24.158, 'eval_steps_per_second': 2.013, 'epoch': 12.31}\n",
      "{'loss': 0.0099, 'learning_rate': 3.0769230769230774e-05, 'epoch': 13.85}\n",
      "{'eval_loss': 0.2814348638057709, 'eval_runtime': 1.3392, 'eval_samples_per_second': 26.882, 'eval_steps_per_second': 2.24, 'epoch': 13.85}\n",
      "{'train_runtime': 525.5696, 'train_samples_per_second': 5.908, 'train_steps_per_second': 0.371, 'train_loss': 1.0324285356662213, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "train_t5(df_train, df_dev, learning_rate = 4e-4, load_best_model_at_end = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect summaries of fine-tuned T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = KP_test['topic'].unique()[0]\n",
    "stance = KP_test['stance'].unique()[1]\n",
    "kps = KP_test[(KP_test['topic'] == topic) & (KP_test['stance'] == stance)]['key_point'].to_list()\n",
    "kps_unique = set(list(kps))\n",
    "args = KP_test[(KP_test['topic'] == topic) & (KP_test['stance'] == stance)]['argument'].to_list()\n",
    "kps_unique_dict = dict(zip(kps_unique, [i for i in range(len(kps_unique))]))\n",
    "gold_cluster_ids = [kps_unique_dict[kps[i]] for i in range(len(kps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mandatory vaccination contradicts basic rights',\n",
       " 'Routine child vaccinations are not necessary to keep children healthy',\n",
       " 'Routine child vaccinations, or their side effects, are dangerous',\n",
       " 'The parents and not the state should decide'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kps_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 7.53it/s]\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015c5d44fe92495bbf8b5437ebab1f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Get Flan-T5 Cluster Sums:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: ['Child vaccination should be mandatory because they cannot catch the virus',\n",
       "  'Child vaccination should be mandatory since they do not have symptoms',\n",
       "  'Child vaccination should not be mandatory because they cannot catch the virus',\n",
       "  'Child vaccination should not be mandatory since they cannot catch the virus',\n",
       "  'Child vaccination should be mandatory as they do not have symptoms'],\n",
       " 1: ['Parents should be permitted to choose whether or not to vaccinate their children',\n",
       "  'Parents should be permitted to choose whether or not to vaccinate their child',\n",
       "  'Parents should be permitted to choose whether to vaccinate their children',\n",
       "  'Parents should be permitted to choose the vaccinations of their children',\n",
       "  'Parents should choose for their children'],\n",
       " 2: ['Vaccination is not mandatory because everyone should choose for themselves whether to be vaccinated',\n",
       "  'It is not mandatory for everyone to be vaccinated',\n",
       "  'It is within the freedom of the people to choose for themselves whether to be vaccinated',\n",
       "  'Religious people should choose for themselves whether to be vaccinated',\n",
       "  'It is within the freedom of the people to decide for themselves whether to vaccinate their children'],\n",
       " 3: ['Child vaccination should not be mandatory as long as it is safe for children',\n",
       "  'Child vaccination should not be mandatory as long as there is evidence of side effects',\n",
       "  'Child vaccination should be avoided as long as it is safe for children',\n",
       "  'Child vaccination should not be mandatory since vaccines can cause harm to the child',\n",
       "  'Child vaccination should be avoided as long as it is not harmful for children']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = get_t5_cluster_sums(args, gold_cluster_ids, topic, stance, num_beams = 6, n = 5)\n",
    "sums"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
