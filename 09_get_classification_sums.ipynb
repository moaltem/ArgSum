{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-based Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritz/miniconda3/envs/AM/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/moritz/miniconda3/envs/AM/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading configuration file config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/moritz/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from argsum import load_test_df, get_smatchtopr_classification_sums, get_barh_classification_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "###########################################################################################################################\n",
    "### Get and evaluate classification based summaries ###\n",
    "###########################################################################################################################\n",
    "\n",
    "def get_classification_sums(df, get_classification_sums_callable, parameter_dict, output_dir = 'results/classification_sums', file_name = None):\n",
    "\n",
    "    # Get unique topics and stances\n",
    "    topics = df['topic'].unique().tolist()\n",
    "    stances = [str(int(sta)) for sta in sorted(df['stance'].unique())]\n",
    "\n",
    "    # Get parameter for iteration\n",
    "    iterate_parameter_names = [item[0] for item in parameter_dict.items() if type(item[1]) == list]\n",
    "    iterate_parameter_values = [parameter_dict[parameter_name] for parameter_name in iterate_parameter_names]\n",
    "    iter_parameter_value_combinations = list(product(*iterate_parameter_values))\n",
    "\n",
    "    # Create empty dict to store the clusters\n",
    "    results = dict(zip(['summaries', 'parameter_names', 'parameter_values'], [dict(zip([str(comb) for comb in iter_parameter_value_combinations], [dict(zip(topics, [dict(zip(stances, [dict(zip(['sum_ids', 'sums', 'runtime'], [None, None, None])) for i in range(len(stances))])) for i in range(len(topics))])) for i in range(len(iter_parameter_value_combinations))])) for i in range(len(['summaries']))] + [iterate_parameter_names, iterate_parameter_values]))\n",
    "\n",
    "    ################################\n",
    "    ### Iterate: topic & stance ####\n",
    "    ################################\n",
    "\n",
    "    for topic_stance in tqdm([(topic, stance) for topic in topics for stance in stances], leave = True, desc = 'topic + stance'):\n",
    "        \n",
    "        topic = topic_stance[0]\n",
    "        stance = topic_stance[1]\n",
    "        mask_topic_stance = (df['topic'] == topic) & (df['stance'] == int(stance))\n",
    "        arguments = df[mask_topic_stance]['argument'].to_list()\n",
    "\n",
    "        ############################\n",
    "        ### Iterate: parameter #####\n",
    "        ############################\n",
    "\n",
    "        for comb in tqdm(iter_parameter_value_combinations, leave = False,  desc = 'summarization parameter'):\n",
    "            iterate_parameter_dict = {**parameter_dict, **dict(zip(iterate_parameter_names, list(comb)))}\n",
    "\n",
    "            ########################\n",
    "            ### Get summaries ######\n",
    "            ########################\n",
    "\n",
    "            start_time = time()\n",
    "            classification_sum_ids, classification_sums = get_classification_sums_callable(arguments, topic = topic, stance = int(stance), **iterate_parameter_dict)\n",
    "            runtime = time() - start_time\n",
    "\n",
    "            if classification_sum_ids != None:\n",
    "                results['summaries'][str(comb)][topic][stance]['sum_ids'] = [int(id) for id in classification_sum_ids]\n",
    "                results['summaries'][str(comb)][topic][stance]['sums'] = classification_sums\n",
    "                results['summaries'][str(comb)][topic][stance]['runtime'] = float(np.round(runtime, 5))\n",
    "\n",
    "    \n",
    "    ########################\n",
    "    ### Save results #######\n",
    "    ########################\n",
    "\n",
    "    if file_name != None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        with open(output_dir + '/' + file_name, 'w') as file:\n",
    "            json.dump(results, file)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArgKP21 = load_test_df('ArgKP21')\n",
    "Debate_test = load_test_df('Debate_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BarH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                       'min_proportion_candidates':[0.1, 0.3], \n",
    "                       'match_scorer_t':[i for i in np.arange(0.75,0.96, 0.025)],\n",
    "                       'final_match_scorer_t':0,\n",
    "                       'use_llm':False\n",
    "                       }\n",
    "\n",
    "barh_results = get_classification_sums(df = ArgKP21, \n",
    "                                       get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                       parameter_dict = barh_parameter_dict, \n",
    "                                       output_dir = 'investigations/3_classification_summaries', \n",
    "                                       file_name = 'ArgKP21_BarH.json'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                            'min_proportion_candidates':[0.1, 0.3], \n",
    "                            'match_scorer_t':[i for i in np.arange(0.75,0.96, 0.025)],\n",
    "                            'final_match_scorer_t':0,\n",
    "                            'use_llm':False\n",
    "                            }\n",
    "\n",
    "barh_results = get_classification_sums(df = Debate_test, \n",
    "                                            get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                            parameter_dict = barh_parameter_dict, \n",
    "                                            output_dir = 'investigations/3_classification_summaries', \n",
    "                                            file_name = f'Debate_test_BarH.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_key_points_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                                  'min_proportion_candidates':0,\n",
    "                                  'match_scorer_t':[i for i in np.arange(0.75,0.96, 0.025)],\n",
    "                                  'final_match_scorer_t':0, \n",
    "                                  'use_llm':'candidates',\n",
    "                                  'sum_token_length':8,\n",
    "                                  'sum_min_num':12,\n",
    "                                  'sum_min_num_plus':8,\n",
    "                                  'temperature':0.5,\n",
    "                                  'frequency_penalty':None,\n",
    "                                  'few_shot':True\n",
    "                                  }\n",
    "\n",
    "barh_key_points_results = get_classification_sums(df = ArgKP21, \n",
    "                                                  get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                                  parameter_dict = barh_key_points_parameter_dict, \n",
    "                                                  output_dir = 'investigations/3_classification_summaries', \n",
    "                                                  file_name = 'ArgKP21_BarH_Candidates.json'\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_key_points_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                                  'min_proportion_candidates':0,\n",
    "                                  'match_scorer_t':[i for i in np.arange(0.75,0.96, 0.025)],\n",
    "                                  'final_match_scorer_t':0, \n",
    "                                  'use_llm':'candidates',\n",
    "                                  'sum_token_length':8,\n",
    "                                  'sum_min_num':12,\n",
    "                                  'sum_min_num_plus':8,\n",
    "                                  'temperature':0.5,\n",
    "                                  'frequency_penalty':None,\n",
    "                                  'few_shot':True\n",
    "                                  }\n",
    "\n",
    "barh_key_points_results = get_classification_sums(df = Debate_test, \n",
    "                                                  get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                                  parameter_dict = barh_key_points_parameter_dict, \n",
    "                                                  output_dir = 'investigations/3_classification_summaries', \n",
    "                                                  file_name = 'Debate_test_BarH_Candidates.json'\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_key_points_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                                  'min_proportion_candidates':0,\n",
    "                                  'match_scorer_t':0.8,\n",
    "                                  'final_match_scorer_t':0, \n",
    "                                  'use_llm':'key_points',\n",
    "                                  'sum_token_length':8,\n",
    "                                  'sum_min_num':[3,4],\n",
    "                                  'sum_min_num_plus':[2,3,4,5,6],\n",
    "                                  'temperature':0.5,\n",
    "                                  'frequency_penalty':None,\n",
    "                                  'few_shot':True\n",
    "                                  }\n",
    "\n",
    "barh_key_points_results = get_classification_sums(df = ArgKP21, \n",
    "                                                  get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                                  parameter_dict = barh_key_points_parameter_dict, \n",
    "                                                  output_dir = 'investigations/3_classification_summaries', \n",
    "                                                  file_name = 'ArgKP21_BarH_Key_Points.json'\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barh_key_points_parameter_dict = {'quality_scorer_t':0.7,\n",
    "                                  'min_proportion_candidates':0,\n",
    "                                  'match_scorer_t':0.8,\n",
    "                                  'final_match_scorer_t':0, \n",
    "                                  'use_llm':'key_points',\n",
    "                                  'sum_token_length':8,\n",
    "                                  'sum_min_num':[3,4],\n",
    "                                  'sum_min_num_plus':[2,3,4,5,6],\n",
    "                                  'temperature':0.5,\n",
    "                                  'frequency_penalty':None,\n",
    "                                  'few_shot':True\n",
    "                                  }\n",
    "\n",
    "barh_key_points_results = get_classification_sums(df = Debate_test, \n",
    "                                                  get_classification_sums_callable = get_barh_classification_sums, \n",
    "                                                  parameter_dict = barh_key_points_parameter_dict, \n",
    "                                                  output_dir = 'investigations/3_classification_summaries', \n",
    "                                                  file_name = 'Debate_test_BarH_Key_Points.json'\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMatchToPr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_parameter_dict = {'quality_scorer_t':0.8,\n",
    "                             'min_proportion_candidates':[0.1, 0.3], \n",
    "                             'match_scorer_pr_t':0.4, \n",
    "                             'damping_factor':0.2, \n",
    "                             'final_match_scorer_t':0,\n",
    "                             'scorer_cands':None, \n",
    "                             'scorer_cands_t':[i for i in np.arange(0.75,0.96, 0.025)], \n",
    "                             'use_llm':False\n",
    "                             }\n",
    "\n",
    "smatchtopr_results = get_classification_sums(df = ArgKP21, \n",
    "                                             get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                             parameter_dict = smatchtopr_parameter_dict, \n",
    "                                             output_dir = 'investigations/3_classification_summaries', \n",
    "                                             file_name = 'ArgKP21_SMatchToPr.json'\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_parameter_dict = {'quality_scorer_t':[0.6, 0.8],\n",
    "                             'match_scorer_pr_t':0.4, \n",
    "                             'damping_factor':0.2, \n",
    "                             'final_match_scorer_t':0,\n",
    "                             'scorer_cands':None, \n",
    "                             'scorer_cands_t':[i for i in np.arange(0.75,0.96, 0.025)], \n",
    "                             'use_llm':False\n",
    "                             }\n",
    "\n",
    "smatchtopr_results = get_classification_sums(df = Debate_test, \n",
    "                                             get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                             parameter_dict = smatchtopr_parameter_dict, \n",
    "                                             output_dir = 'investigations/3_classification_summaries', \n",
    "                                             file_name = 'Debate_test_SMatchToPr.json'\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_key_points_parameter_dict = {'quality_scorer_t':0.8,\n",
    "                                        'match_scorer_pr_t':0.4, \n",
    "                                        'damping_factor':0.2, \n",
    "                                        'final_match_scorer_t':0, \n",
    "                                        'scorer_cands_t':[i for i in np.arange(0.75,0.96, 0.025)], \n",
    "                                        'use_llm':'candidates',\n",
    "                                        'sum_token_length':8,\n",
    "                                        'sum_min_num':8,\n",
    "                                        'sum_min_num_plus':12,\n",
    "                                        'temperature':0.5,\n",
    "                                        'frequency_penalty':None,\n",
    "                                        'few_shot':True\n",
    "                                        }\n",
    "\n",
    "smatchtopr_key_points_results = get_classification_sums(df = ArgKP21, \n",
    "                                                        get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                                        parameter_dict = smatchtopr_key_points_parameter_dict, \n",
    "                                                        output_dir = 'investigations/3_classification_summaries', \n",
    "                                                        file_name = 'ArgKP21_SMatchToPr_Candidates.json'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_key_points_parameter_dict = {'quality_scorer_t':0.8,\n",
    "                                        'match_scorer_pr_t':0.4, \n",
    "                                        'damping_factor':0.2, \n",
    "                                        'final_match_scorer_t':0, \n",
    "                                        'scorer_cands_t':[i for i in np.arange(0.75,0.96, 0.025)], \n",
    "                                        'use_llm':'candidates',\n",
    "                                        'sum_token_length':8,\n",
    "                                        'sum_min_num':8,\n",
    "                                        'sum_min_num_plus':12,\n",
    "                                        'temperature':0.5,\n",
    "                                        'frequency_penalty':None,\n",
    "                                        'few_shot':True\n",
    "                                        }\n",
    "\n",
    "smatchtopr_key_points_results = get_classification_sums(df = Debate_test, \n",
    "                                                        get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                                        parameter_dict = smatchtopr_key_points_parameter_dict, \n",
    "                                                        output_dir = 'investigations/3_classification_summaries', \n",
    "                                                        file_name = 'Debate_test_SMatchToPr_Candidates.json'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_key_points_parameter_dict = {'quality_scorer_t':0.8,\n",
    "                                        'match_scorer_pr_t':0.4, \n",
    "                                        'damping_factor':0.2, \n",
    "                                        'final_match_scorer_t':0, \n",
    "                                        'use_llm':'key_points',\n",
    "                                        'sum_token_length':8,\n",
    "                                        'sum_min_num':[3,4],\n",
    "                                        'sum_min_num_plus':[2,3,4,5,6],\n",
    "                                        'temperature':0.5,\n",
    "                                        'frequency_penalty':None,\n",
    "                                        'few_shot':True\n",
    "                                        }\n",
    "\n",
    "smatchtopr_key_points_results = get_classification_sums(df = ArgKP21, \n",
    "                                                        get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                                        parameter_dict = smatchtopr_key_points_parameter_dict, \n",
    "                                                        output_dir = 'investigations/3_classification_summaries', \n",
    "                                                        file_name = 'ArgKP21_SMatchToPr_Key_Points.json'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smatchtopr_key_points_parameter_dict = {'quality_scorer_t':0.8,\n",
    "                                        'match_scorer_pr_t':0.4, \n",
    "                                        'damping_factor':0.2, \n",
    "                                        'final_match_scorer_t':0, \n",
    "                                        'use_llm':'key_points',\n",
    "                                        'sum_token_length':8,\n",
    "                                        'sum_min_num':[3,4],\n",
    "                                        'sum_min_num_plus':[2,3,4,5,6],\n",
    "                                        'temperature':0.5,\n",
    "                                        'frequency_penalty':None,\n",
    "                                        'few_shot':True\n",
    "                                        }\n",
    "\n",
    "smatchtopr_key_points_results = get_classification_sums(df = Debate_test, \n",
    "                                                        get_classification_sums_callable = get_smatchtopr_classification_sums, \n",
    "                                                        parameter_dict = smatchtopr_key_points_parameter_dict, \n",
    "                                                        output_dir = 'investigations/3_classification_summaries', \n",
    "                                                        file_name = 'Debate_test_SMatchToPr_Key_Points.json'\n",
    "                                                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
